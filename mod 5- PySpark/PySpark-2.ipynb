{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879479c0-818c-472d-b07f-a3e20e797686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataxBootcamp\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1911bab-f190-465b-bad9-5794d1f07732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a small example for lazy evaluation\n",
    "data ={\n",
    "\n",
    "    (\"Alice\",\"IT\",50000),\n",
    "    (\"Bob\",\"HR\",40000),\n",
    "    (\"Charlie\",\"IT\",60000),\n",
    "    (\"David\",\"HR\",55000),\n",
    "    (\"Eve\",\"IT\",70000)\n",
    "}\n",
    "\n",
    "column = [\"Name\",\"Department\",\"Salary\"]\n",
    "df = spark.createDataFrame(data,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf60376-f249-4dcc-9064-c738ccd6f34e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# only on using display a spark job will run\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb974a4-4bd3-4ddb-b12c-f6547892aa0d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753770319222}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select and filter on the dataframe created\n",
    "display(df.select(\"Salary\",\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ba4a25-abbe-44e3-8d61-2eebcc956a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 1 - filter based on salary > 50000\n",
    "display(df.filter(df.Salary > 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08e917ba-bd67-4d4c-94af-7e16f6a8476f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Method 2 - salary > 50000\n",
    "display(df.filter(df['Salary'] > 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d301f0-76ae-4b2e-bcf8-f36a6756eae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter people who are in HR and whose salary > 30000\n",
    "\n",
    "display(df.filter((df['Department'] == 'HR') & (df['Salary'] > 30000))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e894909-1c1e-47c1-b5fc-789d7ba95e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a derived column - column created using anothe column\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# give 10% bonus to everyone\n",
    "df = df.withColumn(\"Bonus\",df.Salary * 0.1)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ef7994-d8aa-42ee-92e1-6c042d7514a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# group by and aggregation\n",
    "\n",
    "# group by department and average of salary\n",
    "\n",
    "df.groupBy(\"Department\").avg(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fe6745-52c5-47ed-8f2c-099847b458a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply multiple aggregations\n",
    "\n",
    "from pyspark.sql.functions import min, max, avg\n",
    "\n",
    "display(df.groupby(\"Department\").agg(\n",
    "    min(\"Salary\").alias(\"Min_salary\"),\n",
    "    max(\"Salary\").alias(\"Max_salary\"),\n",
    "    avg(\"Salary\").alias(\"Avg_salary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be295f84-67c0-4e02-81c8-89a08201c7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join 2 spark dataframes\n",
    "# creating another dataframe\n",
    "\n",
    "dept_data = {(\"HR\",101) , (\"IT\", 102)}\n",
    "dept_column = [\"Department\", \"Dept_id\"]\n",
    "df_dept = spark.createDataFrame(dept_data, dept_column)\n",
    "display(df_dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78eb9864-1cf9-4fdd-a0b1-44c1a81eeb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join df and df_dept\n",
    "df_joined = df.join(df_dept, on=\"Department\", how = \"inner\")\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50aa2f8-2dbf-423c-a87a-312834772f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# transformation vs Action\n",
    "\n",
    "#transformation - only does the operation\n",
    "filtered = df.filter(df.Salary > 40000)\n",
    "\n",
    "#action - result will be visible only on using action statements\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ebd805-36cb-42a8-bf3e-4c30cbbae72e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Student practice assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16be219f-9cf4-454b-8305-f2e584dcb323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸ“‚ Build a DataFrame with the following schema: Name, Department, Salary, Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d868de1-ca68-4960-ab2b-222273846b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"Anurag\", \"IT\", 70000, \"Bangalore\"),\n",
    "    (\"Priya\", \"HR\", 55000, \"Mumbai\"),\n",
    "    (\"Ravi\", \"Finance\", 65000, \"Delhi\"),\n",
    "    (\"Sneha\", \"IT\", 60000, \"Hyderabad\")\n",
    "]\n",
    "\n",
    "columns = [\"Name\", \"Department\", \"Salary\", \"Location\"]\n",
    "\n",
    "df_task = spark.createDataFrame(data, columns)\n",
    "display(df_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94f76eec-1fb3-42f0-949e-14d1b61874dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#task 1 :Filter IT employees with salary > 60K\n",
    "display(df_task.filter((df_task.Department == \"IT\") & (df_task.Salary > 60000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c0a1ca-3f17-4952-bf87-61dd385b5504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 2: Add column \"Hike_Amount\" as 15% of salary\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_task = df_task.withColumn(\"Hike Amount\", df_task.Salary *0.15)\n",
    "display(df_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd6bc8b-f7fa-430f-a505-bedf4d092b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# another approach\n",
    "df_task = df_task.withColumn(\"Hike_Amount\", col(\"Salary\") * 0.15)\n",
    "df_task.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa136ed-8ef0-4e70-b147-e1291361570c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 3: Group by Department and show average salary\n",
    "df_task.groupby(\"Department\").avg(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "882f4a9e-c57c-4bd5-a223-492ce83a7e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark-2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
