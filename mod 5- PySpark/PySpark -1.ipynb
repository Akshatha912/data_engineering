{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b91cdba-2c44-4642-8cb4-a849ecb1822e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(\"Spark session created: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c29bbd15-3b63-4ba2-9f77-d9b18eca569c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac66028-683c-437b-9ad7-1e1a9a8fd792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [('Akshatha', 50000), ('Priya', 60000), ('Anu', 40000), ('Geetha', 30000),('Rani', 70000)]\n",
    "column = ['Name', 'Salary']\n",
    "spark_df = spark.createDataFrame(data, column)\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a41968aa-7417-4fe8-9729-4091eb2b85f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Difference between spark and pandas dataframe - name column is string in spark and object in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5cb6358-9eb4-4c9b-9a32-ab95e92a5476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pandas dataframe\n",
    "import pandas as pd\n",
    "data = [('Akshatha', 50000), ('Priya', 60000), ('Anu', 40000), ('Geetha', 30000),('Rani', 70000)]\n",
    "columns = ('Name','Salary')\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2c7143-bf2c-439e-b5b8-087cd0779c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5d23b4-47d2-4e26-9338-e0ffb259113c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Counts in df: \", spark_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b57f682d-6471-4721-944b-84c4867c1889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Real-Life Use Case**\n",
    "\n",
    " #These DataFrames are used for:\n",
    "\n",
    "Ingesting data from files\n",
    "\n",
    "Transforming at scale\n",
    "\n",
    "Writing to Delta Tables or Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e8a0a1d-2785-42bf-b94a-706df9fd49bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Add Data Types \n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True), # structType -to enforce data type .name should only be in string and salary in integer type.\n",
    "    StructField(\"Salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_typed = spark.createDataFrame(data, schema=schema)\n",
    "df_typed.show()\n",
    "df_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b955ba66-90bc-46ab-960d-c0242afe83ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Q: What's the difference between SparkSession and SparkContext?\n",
    "\n",
    "âœ… Answer:\n",
    "\n",
    "SparkSession is the unified entry point from Spark 2.0 onwards.\n",
    "It internally manages SparkContext, SQLContext, and HiveContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "069f3b9a-8422-417a-b0be-6b5c13c722de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a spark dataframe using json data\n",
    "\n",
    "json_data = [\n",
    "    {\"Name\": \"David\", \"Salary\": 70000},\n",
    "    {\"Name\": \"Eva\", \"Salary\": 65000}\n",
    "]\n",
    "\n",
    "df_json=spark.createDataFrame(json_data)\n",
    "display(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c016cadc-991b-4392-8189-fc9f1dae407c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark -1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
